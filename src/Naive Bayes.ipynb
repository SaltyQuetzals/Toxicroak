{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/quontas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import ComplementNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "from functions import load_bad_words, load_ethnic_slurs, build_data_path, print_report, run_on_test_data\n",
    "from constants import LABEL_COLS\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAD_WORDS = set(load_bad_words())\n",
    "ETHNIC_SLURS = set(load_ethnic_slurs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_path = build_data_path('augmented_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(training_data_path)\n",
    "\n",
    "X = df['comment_text']\n",
    "y = df[LABEL_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would you like to perform an exhaustive search? NOTE: This will take several hours.\n",
      "Please enter \"yes\" or \"no\".no\n"
     ]
    }
   ],
   "source": [
    "clf = OneVsRestClassifier(ComplementNB())\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents='ascii', stop_words='english', ngram_range=(1, 1), norm='l2')\n",
    "bad_word_counter = CountVectorizer(vocabulary=BAD_WORDS)\n",
    "slur_counter = CountVectorizer(vocabulary=ETHNIC_SLURS)\n",
    "union = make_union(tfidf, bad_word_counter, slur_counter)\n",
    "\n",
    "pipeline = make_pipeline(union, clf)\n",
    "\n",
    "optimizer = pipeline\n",
    "\n",
    "print('Would you like to perform an exhaustive search? NOTE: This will take several hours.')\n",
    "autotune_hyperparameters = input('Please enter \"yes\" or \"no\".')\n",
    "\n",
    "# Auto-tune hyperparameters\n",
    "while autotune_hyperparameters.lower() not in ['yes', 'no']:\n",
    "    autotune_hyperparameters = input('Please enter \"yes\" or \"no\".')\n",
    "if autotune_hyperparameters == 'yes':\n",
    "    parameters = {\n",
    "        'featureunion__tfidfvectorizer__lowercase': [True, False],\n",
    "        'featureunion__tfidfvectorizer__strip_accents': [None, 'ascii', 'unicode'],\n",
    "        'featureunion__tfidfvectorizer__stop_words': [None, 'english'],\n",
    "        'featureunion__tfidfvectorizer__norm': [None, 'l1', 'l2'],\n",
    "        'featureunion__tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)],\n",
    "        'onevsrestclassifier__estimator__alpha': [0.001, 0.01, 0.1, 1.0],\n",
    "        \n",
    "        'onevsrestclassifier__estimator__norm': [True, False],\n",
    "        'featureunion__tfidfvectorizer__max_features': [1000, 5000, 10000, None]\n",
    "    }\n",
    "    optimizer = GridSearchCV(pipeline, parameters, scoring='roc_auc', verbose=3)\n",
    "\n",
    "optimizer.fit(X_train, y_train)\n",
    "y_predictions = optimizer.predict(X_valid)\n",
    "\n",
    "# best_estimator_ = optimizer.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION RESULTS:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quontas/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/quontas/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "        toxic       0.80      0.67      0.73     20216\n",
      " severe_toxic       0.27      0.75      0.40      2113\n",
      "      obscene       0.71      0.79      0.74     11118\n",
      "       threat       0.04      0.13      0.07       651\n",
      "       insult       0.61      0.69      0.65     10351\n",
      "identity_hate       0.19      0.32      0.24      1867\n",
      "\n",
      "    micro avg       0.61      0.68      0.64     46316\n",
      "    macro avg       0.44      0.56      0.47     46316\n",
      " weighted avg       0.68      0.68      0.67     46316\n",
      "  samples avg       0.05      0.06      0.05     46316\n",
      "\n",
      "Class-wise AUC-ROC (Kaggle) [0.82422703 0.8625404  0.88341197 0.56086954 0.83522456 0.65600357]\n",
      "Overall AUC-ROC (Kaggle) 0.770379510275617\n"
     ]
    }
   ],
   "source": [
    "print_report(y_valid, y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING RESULTS:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        toxic       0.50      0.75      0.60      6090\n",
      " severe_toxic       0.10      0.77      0.18       367\n",
      "      obscene       0.44      0.79      0.57      3691\n",
      "       threat       0.01      0.07      0.02       211\n",
      "       insult       0.39      0.68      0.50      3427\n",
      "identity_hate       0.15      0.41      0.22       712\n",
      "\n",
      "    micro avg       0.38      0.72      0.49     14498\n",
      "    macro avg       0.27      0.58      0.35     14498\n",
      " weighted avg       0.42      0.72      0.53     14498\n",
      "  samples avg       0.06      0.07      0.06     14498\n",
      "\n",
      "Class-wise AUC-ROC (Kaggle) [0.83572849 0.86679081 0.86317551 0.52361712 0.81060037 0.69029651]\n",
      "Overall AUC-ROC (Kaggle) 0.7650348029135587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quontas/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/quontas/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "run_on_test_data(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top10(vectorizer, clf, class_labels):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "        print(\"%s:\\n\\t%s\\n\" % (class_label,\n",
    "              \"\\n\\t\".join(feature_names[j].split('__')[-1] for j in top10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic:\n",
      "\t2123145146\n",
      "\tkundad\n",
      "\tkunstruktive\n",
      "\tkunt\n",
      "\tkupla\n",
      "\tkurang\n",
      "\tyammer\n",
      "\tfollarte\n",
      "\tfuckyourself\n",
      "\tcrackhead\n",
      "\n",
      "severe_toxic:\n",
      "\tstomes\n",
      "\tstikin\n",
      "\tcaspa\n",
      "\tanastal1111you\n",
      "\tancest\n",
      "\tancestryearly\n",
      "\tancestryerigate\n",
      "\tada_at\n",
      "\tcartuchos\n",
      "\thomelan\n",
      "\n",
      "obscene:\n",
      "\tachivements\n",
      "\tachmed\n",
      "\tachsehole\n",
      "\tkcik\n",
      "\tsexmist\n",
      "\tbritch\n",
      "\tbritbarb\n",
      "\tkatzrin\n",
      "\tzigabo\n",
      "\tfollarte\n",
      "\n",
      "threat:\n",
      "\tm45terbate\n",
      "\tma5terb8\n",
      "\tma5terbate\n",
      "\tmaster-bate\n",
      "\tmasterb8\n",
      "\tmasterbat*\n",
      "\tmasterbat3\n",
      "\tteeeccccctooooniiiiiicccccc\n",
      "\thawkinghttp\n",
      "\tzigabo\n",
      "\n",
      "insult:\n",
      "\tfaggots129\n",
      "\tislantic\n",
      "\tsnigbrook\n",
      "\tfurfag\n",
      "\tfortuijn\n",
      "\t66185192207\n",
      "\tlibtard\n",
      "\tonanizing\n",
      "\tcrackhead\n",
      "\tsuberbia\n",
      "\n",
      "identity_hate:\n",
      "\tgomnna\n",
      "\tcloserlookonsyria\n",
      "\tnawmean\n",
      "\tgoddammed\n",
      "\tclubz\n",
      "\tgoains\n",
      "\tnebracka\n",
      "\tnegrate\n",
      "\tuos\n",
      "\tzigabo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformers = optimizer.named_steps.featureunion\n",
    "classifier = optimizer.named_steps.onevsrestclassifier\n",
    "\n",
    "print_top10(transformers, clf, LABEL_COLS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
