{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/quontas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import ComplementNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "from functions import load_bad_words, load_ethnic_slurs, build_data_path, print_report, run_on_test_data\n",
    "from constants import LABEL_COLS\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PorterStemmerTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.ps = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.ps.stem(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAD_WORDS = set(load_bad_words())\n",
    "ETHNIC_SLURS = set(load_ethnic_slurs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_path = build_data_path('augmented_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(training_data_path)\n",
    "\n",
    "X = df['comment_text']\n",
    "y = df[LABEL_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would you like to perform an exhaustive search? NOTE: This will take several hours.\n",
      "Please enter \"yes\" or \"no\".no\n"
     ]
    }
   ],
   "source": [
    "clf = OneVsRestClassifier(ComplementNB())\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents='ascii', stop_words='english', ngram_range=(1, 3), norm='l2')\n",
    "bad_word_counter = CountVectorizer(vocabulary=BAD_WORDS)\n",
    "slur_counter = CountVectorizer(vocabulary=ETHNIC_SLURS)\n",
    "union = make_union(tfidf, bad_word_counter, slur_counter)\n",
    "\n",
    "pipeline = make_pipeline(union, clf)\n",
    "\n",
    "optimizer = pipeline\n",
    "\n",
    "print('Would you like to perform an exhaustive search? NOTE: This will take several hours.')\n",
    "autotune_hyperparameters = input('Please enter \"yes\" or \"no\".')\n",
    "\n",
    "# Auto-tune hyperparameters\n",
    "while autotune_hyperparameters.lower() not in ['yes', 'no']:\n",
    "    autotune_hyperparameters = input('Please enter \"yes\" or \"no\".')\n",
    "if autotune_hyperparameters == 'yes':\n",
    "    parameters = {\n",
    "        'featureunion__tfidfvectorizer__lowercase': [True, False],\n",
    "        'featureunion__tfidfvectorizer__strip_accents': [None, 'ascii', 'unicode'],\n",
    "        'featureunion__tfidfvectorizer__stop_words': [None, 'english'],\n",
    "        'featureunion__tfidfvectorizer__norm': [None, 'l1', 'l2'],\n",
    "        'featureunion__tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)],\n",
    "        'featureunion__tfidfvectorizer__tokenizer': [None,]\n",
    "        'onevsrestclassifier__estimator__alpha': [0.001, 0.01, 0.1, 1.0],\n",
    "        \n",
    "        'onevsrestclassifier__estimator__norm': [True, False],\n",
    "        'featureunion__tfidfvectorizer__max_features': [1000, 5000, 10000, None]\n",
    "    }\n",
    "    optimizer = GridSearchCV(pipeline, parameters, scoring='roc_auc', verbose=3)\n",
    "\n",
    "optimizer.fit(X_train, y_train)\n",
    "y_predictions = optimizer.predict(X_valid)\n",
    "\n",
    "# best_estimator_ = optimizer.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION RESULTS:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quontas/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/quontas/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "        toxic       0.96      0.55      0.69     20257\n",
      " severe_toxic       0.64      0.28      0.39      2099\n",
      "      obscene       0.92      0.63      0.75     11086\n",
      "       threat       0.78      0.03      0.06       643\n",
      "       insult       0.86      0.47      0.61     10326\n",
      "identity_hate       0.86      0.09      0.16      1924\n",
      "\n",
      "    micro avg       0.91      0.51      0.66     46335\n",
      "    macro avg       0.84      0.34      0.45     46335\n",
      " weighted avg       0.91      0.51      0.64     46335\n",
      "  samples avg       0.05      0.04      0.04     46335\n",
      "\n",
      "Class-wise AUC-ROC (Kaggle) [0.77163422 0.6378582  0.81585634 0.51631542 0.73341973 0.54541109]\n",
      "Overall AUC-ROC (Kaggle) 0.6700824997884114\n"
     ]
    }
   ],
   "source": [
    "print_report(y_valid, y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING RESULTS:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        toxic       0.73      0.51      0.60      6090\n",
      " severe_toxic       0.22      0.19      0.20       367\n",
      "      obscene       0.75      0.53      0.62      3691\n",
      "       threat       0.00      0.00      0.00       211\n",
      "       insult       0.70      0.34      0.46      3427\n",
      "identity_hate       0.42      0.09      0.15       712\n",
      "\n",
      "    micro avg       0.70      0.44      0.54     14498\n",
      "    macro avg       0.47      0.28      0.34     14498\n",
      " weighted avg       0.69      0.44      0.53     14498\n",
      "  samples avg       0.05      0.04      0.04     14498\n",
      "\n",
      "Class-wise AUC-ROC (Kaggle) [0.74552029 0.59078017 0.75730547 0.49944329 0.66788759 0.54424834]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quontas/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/quontas/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AUC-ROC (Kaggle) 0.6341975255217371\n"
     ]
    }
   ],
   "source": [
    "run_on_test_data(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top10(vectorizer, clf, class_labels):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "        print(\"%s:\\n\\t%s\\n\" % (class_label,\n",
    "              \"\\n\\t\".join(feature_names[j].split('__')[-1] for j in top10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic:\n",
      "\tviolate rules tard\n",
      "\tpedia look\n",
      "\tpedia hate wikipedia\n",
      "\tpedia hate\n",
      "\tpedia group illiterate\n",
      "\tpedia group\n",
      "\tpedia greek geak\n",
      "\tpedia greek\n",
      "\tpedia fully\n",
      "\tzigabo\n",
      "\n",
      "severe_toxic:\n",
      "\tappreciate chinese\n",
      "\tbooks weirdest\n",
      "\thoax hahahahahahahaha\n",
      "\tho wikipedia shove\n",
      "\tbooks working\n",
      "\tbooks working biggest\n",
      "\tbooks working largest\n",
      "\tappreciate china\n",
      "\tbooks weirdest okay\n",
      "\tzigabo\n",
      "\n",
      "obscene:\n",
      "\tdisrupt valuable\n",
      "\tinch dick asshole\n",
      "\tinch dick gonna\n",
      "\tsources article particular\n",
      "\tphenomenon bound racist\n",
      "\tphenomenon bound\n",
      "\tanti seMites palestinians\n",
      "\tinch gay\n",
      "\tlittle man arse\n",
      "\tzigabo\n",
      "\n",
      "threat:\n",
      "\tkidnap rape family\n",
      "\twilling password\n",
      "\twilling password going\n",
      "\tboy boy think\n",
      "\tfucking face family\n",
      "\tfucking fag fag\n",
      "\tnegro heil\n",
      "\tnegro heil hitler\n",
      "\tfucking 40\n",
      "\tzigabo\n",
      "\n",
      "insult:\n",
      "\tlooking oit shut\n",
      "\tlooking oit\n",
      "\tlooking nigger cares\n",
      "\tlooking nigger\n",
      "\tlooking naw going\n",
      "\tlooking naw\n",
      "\tlooking motherfucking pin\n",
      "\tlooking motherfucking nip\n",
      "\tdesignthesyline clearly\n",
      "\tzigabo\n",
      "\n",
      "identity_hate:\n",
      "\tfood language\n",
      "\tfood stop climing\n",
      "\tfood shit nigger\n",
      "\tcents hour\n",
      "\tcents hour damned\n",
      "\tcents hour fucking\n",
      "\taddress mean penis\n",
      "\taddress mean\n",
      "\tfood stop climbing\n",
      "\tzigabo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformers = optimizer.named_steps.featureunion\n",
    "classifier = optimizer.named_steps.onevsrestclassifier\n",
    "\n",
    "print_top10(transformers, clf, LABEL_COLS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
